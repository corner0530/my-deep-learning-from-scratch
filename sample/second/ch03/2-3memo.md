# word2vec

## one-hot 表現

- ベクトルの要素の中で 1 つだけが 1 で残りがすべて 0 であるようなベクトル → これを入力層とする
  - これに重みを掛けることは重みの行ベクトル(=分散表現)を「抜き出す」ことに相当する

## CBOW(continuous bag-of-words)

- コンテキストを入力し，ターゲットを推測することを目的とした NN
  - 入力層: コンテキスト(one-hot 表現)を単語数だけ入力層として別々に用意する
  - 中間層: それらに共通の重みで全結合層に入力し平均を取る
  - 出力層: 中間層とは別の重みで全結合層に入力し出力をスコアとする(Softmax 関数を適用することで確率とできる)
  - 入力層 → 中間層の全結合層の重み(の各行)が(各)単語の分散表現となる → 意味がエンコードされる
    - 中間層 → 出力層の全結合層の重み(の各列)にも(各)単語の分散表現が格納されるが，こちらはあまりに使われない
    - 全結合層は MatMul レイヤ (バイアスがないため)
  - 学習: Softmax とクロスエントロピー誤差を用いるだけ

## 学習データ

- コーパスからコンテキストとターゲットを作成する

## 補足

### CBOW モデルと確率

$w_1,w_2,\ldots,w_T$という単語列について，ウィンドウサイズが 1 のときにコンテキストとして$w_{t-1}$と$w_{t+1}$が与えられたときのターゲットが$w_t$の確率は$P(w_t\mid w_{t-1},w_{t+1})$で，これをモデル化している．

1 つのサンプルデータに関数の損失関数(**負の対数尤度**)は，

$$
L=-\log P(w_t\mid w_{t-1},w_{t+1})
$$

で，これをコーパス全体に拡張すると，

$$
L=-\frac{1}{T}\sum_{t-1}^T\log P(w_t\mid w_{t-1},w_{t+1})
$$

となり，これを損失関数とする．

### skip-gram モデル

- CBOW で扱うコンテキストとターゲットを逆転させたモデル(中央の単語から周囲の複数ある単語を推測する)
  - $P(w_{t-1},w_{t+1}\mid w_t)$をモデル化したもの
- ネットワークは，入力層が 1 つ，出力層がコンテキストの数だけ存在する．
- $P(w_{t-1},w_{t+1}\mid w_t)=P(w_{t-1}\mid w_t)P(w_{t+1}\mid w_t)$の条件付独立(コンテキストの単語間に関連性がないこと)を仮定して，1 つのデータに対する負の対数尤度は，

  $$
  L=-\left(\log P\left(w_{t-1}\mid w_t\right)+\log P\left(w_{t+1}\mid w_t\right)\right)
  $$

  となり，コーパス全体の損失関数は

  $$
  L=-\frac{1}{T}\sum_{t=1}^T\left(\log P\left(w_{t-1}\mid w_t\right)+\log P\left(w_{t+1}\mid w_t\right)\right)
  $$

- skip-gram モデルは CBOW モデルに比べて，単語の分散表現の精度が優れているが学習速度は遅い．

### カウントベース vs. 推論ベース

- 新しい単語の追加による単語の分散表現の更新の場合，カウントベースは共起行列の作成から行うが，推論ベースでは再学習を行えるので，推論ベースの方が効率的．
- 単語の分散表現の性質や精度については，カウントベースでは単語の類似性が，word2vec ではそれに加えて単語間のパターンをとらえられる．
- しかし優劣はつけられない
- また，skip-gram と Negative Sampling を用いたモデルは，コーパス全体の共起行列に対して特別な行列分解をしているのと同じ
- さらにカウントベースと推論ベースを合わせた GloVe もある
