# RNN による文章生成

- **seq2seq**: 時系列データを別の時系列データに変換するモデルの 1 つで 2 つの RNN を利用する
  - Encoder-Decoder モデル(Encoder でエンコードした情報から目的とする文章を Decoder で生成する)
    - Encoder は Embedding レイヤ →LSTM レイヤを用いて隠れ状態ベクトル$\bm{h}$に変換する
    - Decoder は LSTM レイヤで$\bm{h}$を受け取って文章生成を行う
- トイ・プロブレム: 機械学習を評価するために作られた簡単な問題
- 可変長の時系列データを扱う →**パディング**を行う(無効なデータで埋めてデータの長さを均一にする)
  - 無効なデータを処理するため，パディング専用の処理を seq2seq に加える必要がある
    - Decoder でパディングが入力されたときには損失の結果に計上しないようにする ←Softmax with Loss レイヤに「マスク」を追加する
    - Encoder でパティングが入力されたときには前時刻の入力をそのまま出力
- 改良
  - 入力データの反転(Reverse): 学習の進みが早くなり，精度もよくなる(勾配の伝播がスムーズになる(反転すると対応関係にある返還後の単語との距離が近くなることが多い)ため)
  - **覗き見(Peeky)**: Encoder の出力$\bm{h}$を Decoder の他のレイヤにも与える
- 応用
  - 機械翻訳・自動要約・質疑応答・メールの自動返信
  - アルゴリズムの学習
  - **イメージキャプション**: 画像を文章へ変換する(Encoder を CNN に置き換える)
