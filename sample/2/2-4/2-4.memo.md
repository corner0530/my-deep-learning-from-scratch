# word2vec の高速化

- CBOW モデルにおいて，語彙数が増えると以下の計算に多くの時間を要する
  - 入力層の one-hot 表現と重み行列$W_{in}$の積による計算(Embedding レイヤで解決)
  - 中間層と重み行列$W_{out}$の積及び Softmax レイヤの計算(Negative Sampling で解決)

## Embedding レイヤ

- 重みパラメータから「単語 ID に該当する行(ベクトル)」を抜き出すレイヤ
  - 単語の埋め込みが格納される

## Negative Sampling

- 「多値分類」を「二値分類」に近似する
  - コンテキストから求めた単語がターゲットと一致するかどうか
- 中間層と出力層の重み行列の積 = 出力層の重み行列からターゲットに対応する列(単語ベクトル)を抽出して中間層のニューロンと内積をとる
  - これをスコアとしてシグモイド関数を適用
- 正例については以上のように学習する．同時に負例についてはいくつかサンプリングしてそれについても損失を求め，それらについての損失を足し合わせて最終的な損失とする．
- 負例のサンプリングは，コーパス中でよく使われる単語をより多く抽出する．
  - コーパス中の各単語の出現回数を以下の確率分布で表す($P\left(w_i\right)$は$i$番目の単語の確率)．
$$
P^\prime\left(w_i\right)=\frac{P\left(w_i\right)^{0.75}}{\sum_j^nP\left(w_j\right)^{0.75}}
$$
