# word2vec の高速化

- CBOW モデルにおいて，語彙数が増えると以下の計算に多くの時間を要する
  - 入力層の one-hot 表現と重み行列$W_{in}$の積による計算(Embedding レイヤで解決)
  - 中間層と重み行列$W_{out}$の積及び Softmax レイヤの計算(Negative Sampling で解決)

## Embedding レイヤ

- 重みパラメータから「単語 ID に該当する行(ベクトル)」を抜き出すレイヤ
  - 単語の埋め込みが格納される

## Negative Sampling

- 「多値分類」を「二値分類」に近似する
  - コンテキストから求めた単語がターゲットと一致するかどうか
- 中間層と出力層の重み行列の積 = 出力層の重み行列からターゲットに対応する列(単語ベクトル)を抽出して中間層のニューロンと内積をとる
  - これをスコアとする
  - 全ての単語を対象に計算するのではなく，ある単語のスコアを出してそれにシグモイド関数を適用
