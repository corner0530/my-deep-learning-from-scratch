# リカレントニューラルネットワーク(RNN)

- **フィードフォワード**は流れが一方向のネットワークで，時系列データをうまく扱えないため，RNN を用いる
- **言語モデル**: 単語の並びに対して，それがどれだけ起こりえるのかを確率で評価するモデル
  $$
  P\left(w_1,\ldots,w_m\right)=\prod_{t=1}^mP\left(w_t\mid w_1,\ldots,w_{t-1}\right)
  $$
  - ここで右辺の事後確率は，対象の単語より左の全ての単語をコンテキストとしたときの確率の総乗
- RNN はループする経路を持ちデータが循環する
  - 時刻を$t$として$x_t$を入力とし，$h_t$が出力される
- ループを展開すると，フィードフォワード型と同じ形ですべて同じレイヤにあることになる
- 入力$x$を出力$h$に変換する重み$W_x$と 1 つ前の RNN の出力を次の時刻の出力に変換する重み$W_h$，バイアス$b$を用いて以下のように表せる
  $$
  h_t=\tanh\left(h_{t-1}W_h+x_tW_x+b\right)
  $$
  - なお，$h_t$を**隠れ状態**(**隠れ状態ベクトル**)と呼ばれる
- **Backpropagation Through Time(BPTT)**: 時間方向に展開したニューラルネットワークの誤差逆伝播法
  - 時系列データが長くなると計算リソースが増加する
- **Truncated BPTT**: ネットワークのつながり(逆伝播のみ)を適当な長さで断ち切ってネットワークを作る BPTT
  - データをシーケンシャルに与える
  - ミニバッチ学習を行うには，(サンプル数)/(バッチ数)個だけオフセットを取る
- **パープレキシティ**: 言語モデルの予測性能の良さを評価する指標として良く用いられる「確率の逆数」=平均分岐数で，データの個数を$N$個，$\mathbf{t}_n$を one-hot vector の正解ラベル，$t_{nk}$を$n$個目のデータの$k$番目の値，$y_{nk}$を確率分布として，以下で表される
  $$
  \begin{align*}
    L&=-\frac{1}{N}\sum_n\sum_kt_{nk}\log y_{nk}\\
    \text{perplexity}&=e^L
  \end{align*}
  $$
