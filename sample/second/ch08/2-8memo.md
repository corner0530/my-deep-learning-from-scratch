# Attention

- **Attention(注意機構)**: 必要な情報に注意を向けさせる
- seq2seq では時系列データを固定長のベクトルに変換するが，これを改良する
- Encoder では各時刻の LSTM レイヤの隠れ状態ベクトルを全て利用する(ベクトルの長さは時刻により変わる)
  - 各時刻の隠れ状態にはその時刻で入力された単語の情報が多く含まれる
- Decoder では，Encoder の隠れ状態ベクトル全てを活用するため，以下の改良を行う
  1. 入力と出力でどの単語が関連しているかの対応関係(**アライメント**)を学習させるため，対応関係にある元の単語の情報を選び出すように注意を向けさせる
  - ただし実際は選び出すのではなく全ての単語について重みづけし，この重みづけ和を足し合わせてコンテキストベクトルとする
  2. 1 の重みの学習は以下のように行う
  - 各単語について，LSTM レイヤの隠れ状態ベクトルと，Encoder の各単語の隠れ状態ベクトルの類似度を内積で算出しスコアとする
  3. Encoder が出力する各単語のベクトルに対し，2 を用いて注意を払って各単語の重みを求め，この重みを用いて 1 で重みづけ和を求め，これをコンテキストベクトルとする．そしてこのコンテキストベクトルを LSTM レイヤの入力に追加する
- 翻訳用のデータセットとして「WTM」などが有名
- 双方向 LSTM/双方向 RNN:逆方向に処理する LSTM レイヤを追加し，順方向と連結する．これにより両方向からの情報を集約できる
- Attention レイヤの場所は LSTM レイヤと Affine レイヤの間以外に，次の時刻の LSTM レイヤの入力の前などにおくこともある
- seq2seq において LSTM レイヤを多層にすると表現力がより高くなる(但し Encoder と Decoder で層数は同じにするのが一般的)
- **skip コネクション**: 層を深くする際に層をまたいで接続するテクニック．接続部で 2 つの出力が加算される
- 応用
  - **GNMT**(Google Neural Machine Translation): 機械翻訳
  - **Transformer**: RNN は並列的に計算できないため RNN ではなく Attention を使って処理する
    - **Self-Attention**: 1 つの時系列データを対象とした Attention であり，1 つの時系列データ内において各要素が他の要素に対してどのような関連性があるのかを見るもの
  - **NTM**(Neural Turing Machine): ニューラルネットワークを外部メモリを利用
    - Encoder が必要な情報をメモリに書き込み，Decoder はそのメモリにある情報から必要な情報を読み込んでいると解釈すると，コンピュータのメモリ操作を NN で再現できそう
