# word2vec の高速化

- CBOW モデルにおいて，語彙数が増えると以下の計算に多くの時間を要する
  - 入力層の one-hot 表現と重み行列$W_{in}$の積による計算(Embedding レイヤで解決)
  - 中間層と重み行列$W_{out}$の積及び Softmax レイヤの計算(Negative Sampling で解決)

## Embedding レイヤ

- 重みパラメータから「単語 ID に該当する行(ベクトル)」を抜き出すレイヤ
  - 単語の埋め込みが格納される

## Negative Sampling

- 「多値分類」を「二値分類」に近似する
  - コンテキストから求めた単語がターゲットと一致するかどうか
- 中間層と出力層の重み行列の積 = 出力層の重み行列からターゲットに対応する列(単語ベクトル)を抽出して中間層のニューロンと内積をとる
  - これをスコアとしてシグモイド関数を適用
- 正例については以上のように学習する．同時に負例についてはいくつかサンプリングしてそれについても損失を求め，それらについての損失を足し合わせて最終的な損失とする．
- 負例のサンプリングは，コーパス中でよく使われる単語をより多く抽出する．
  - コーパス中の各単語の出現回数を以下の確率分布で表す($P\left(w_i\right)$は$i$番目の単語の確率)．
    $$
    P^\prime\left(w_i\right)=\frac{P\left(w_i\right)^{0.75}}{\sum_j^nP\left(w_j\right)^{0.75}}
    $$

## 補足

### アプリケーション

- **転移学習**: ある分野で学習したデータを別の分野にも適用できる
  - 大きなコーパスで先に学習し，その分散表現を個別のタスクで利用する
- 1 つの単語を固定長のベクトルに変換できる
  - 文章に対しても同様に変換でき，最も単純な方法として bag-of-words がある
    - 各単語を分散表現に変換し，それらの総和を求める
    - RNN を用いてもできる
  - 単語や文章を固定長のベクトルに変換することで，NN や SVM など一般的な機械学習の手法に適用できる

### 単語ベクトルの評価方法

- 現実的なアプリケーションでは，単語の分散表現の学習と，特定の問題について分類を行うシステムの学習の 2 段階の学習を行うことが多く，チューニングに多くの時間がかかる．
- そのため，単語の「類似性」や「類推問題」で評価することが一般的．
  - 人が作成した単語類似度の評価セットを用いて比較
  - 類推問題による評価でもその正解率を用いる
    - モデルによって精度が異なる(コーパスに応じて最適なモデルを選ぶ)
    - コーパスが大きいほど良い結果になる(ビッグデータは常に望まれる)
    - 単語ベクトルの次元数は適度な大きさが必要(大きすぎても精度が悪くなる)
